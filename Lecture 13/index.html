<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content=
      "width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Computational Physics Lecture 13</title>
    <link rel="stylesheet" href="../dist/reset.css">
    <link rel="stylesheet" href="../dist/reveal.css">
    <link rel="stylesheet" href="../dist/theme/serif.css"
      id="theme">
    <!-- <link rel="stylesheet" href="./dist/theme/night.css" id="theme"> -->
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href=
      "../plugin/highlight/monokai.css" id="highlight-theme">
    <link rel="stylesheet" href=
      "../css/style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Boundary Value Problems for ODEs</h2>
        </section>

        <section>
          <h2>Boundary Value Problems</h2>
          <p>A boundary value problem (BVP) is an ODE with boundary conditions</p>
          $$
          \begin{align*}
          \frac{d^2y}{dx^2} &= f(x,y,y') \\
          y(a) &= \alpha \\
          y(b) &= \beta
          \end{align*}
          $$
        </section>

        <section>
          <h2>Shooting Method</h2>
          <p>Last time, we discussed the shooting method for converting boundary
            value problems into initial value problems.</p>
          <ol>
            <li class="fragment">Start with an initial guess of $y'(a)$. Compute
              $y(b)$ using an ODE solver like RK45 Dormand Prince. We call this
              $y_1$.</li>
            <li class="fragment">Modify the initial guess with a small
              increment, $y'(a) + h$. Compute $y(b)$ again. We call this $y_2$.</li>
            <li class="fragment">The "derivative" of this procedure can now be
              estimated as $F' = (y_2 - y_1) / h$.</li>
            <div class="fragment">
              <li>Form a new guess of $y'(a)$ using Newton's rule:</li>
              $$
              y'_\mathrm{new} = y'_\mathrm{old} - \frac{y_1}{F'}
              $$
            </div>
            <li class="fragment">Repeat until $|y(b) - \beta|$ is smaller than a given tolerance.</li>
          </ol>
        </section>

        <section>
          <h2>Shooting Method</h2>
          <p>The shooting method is easy to implement, but does have a few inherent drawbacks:</p>
          <ul>
            <li class="fragment">The method completely fails when there is a singularity in the domain
              $[a,b]$.</li>
            <li class="fragment">When the ODE is sensitive to initial conditions
              (e.g. chaotic systems), the method may become unstable to numerical error accumulation.</li>
            <li class="fragment">The method is not easily generalized to systems
              of ODEs with more than 1 unknown functions.</li>
          </ul>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>
            Discretizing the domain $[a,b]$ into $N + 1$ points $x_i$ with
            $x_0 = a$ and $x_N = b$, we can in general rewrite the ODE as a
            <em>Finite Difference Equation</em>:
          </p>
          $$
          \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} = f\left(x_i,y_i,\frac{y_{i+1}-y_{i-1}}{2h}\right),\quad i = 1,2,\ldots,N-1
          $$
          <p>
            where $h = (b-a)/N$ is the step size. This is a system of $N-1$ equations
            for the $N-1$ unknowns $y_1,\ldots,y_{N-1}$.
          </p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Approximating the derivative in this way is called "finite
            difference method". It is analogous to using the definition of the
            derivative but not taking the $h\to 0$ limit.
            $$
            f'(x_i) \approx \frac{f(x_i+h) - f(x_i)}{h} = \frac{f(x_{i+1}) - f(x_i)}{h}
            $$
          </p>
          <p class="fragment">This expression is called "forward difference"
            since it is computed using the values of $f$ at $x_i$ and $x_{i+1}$.
            In the limit $h \to 0$, we expect the approximation to converge to
            the value of the derivative. The question is how quickly does it
            converge?</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Let's estimate the error due to forward difference. Start from the
            regular Taylor explansion:</p>
          $$
          f(x_i + h) = f(x_i) + hf'(x_i) + \frac{1}{2}h^2f''(x_i) + O(h^3)
          $$
          <p>We can rearrange this to get $f'(x_i)$ on the left hand side:</p>
          $$
          f'(x_i) = \frac{f(x_i+h) - f(x_i)}{h} - \frac{1}{2}hf''(x_i) + O(h^2)
          $$
          <p>The first term is our forward difference expression, therefore the
            leading error is $\varepsilon \sim O(h)$. This is a first order
            method.</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>We can achieve second order accuracy by using a slightly different formula:</p>
          $$
          f'(x_i) \approx \frac{f(x_i+h) - f(x_i-h)}{2h} = \frac{f(x_{i+1}) - f(x_{i-1})}{2h}
          $$
          <p>This scheme is called <em>central difference</em>, since it
            symmetrically involves $x_{i-1}$ and $x_{i+1}$.</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>Expanding both $f(x_i+h)$ and $f(x_i-h)$ in Taylor series, we get:</p>
          $$
          \begin{align}
          f(x_i+h) &= f(x_i) + hf'(x_i) + \frac{1}{2}h^2f''(x_i) + \frac{1}{6}h^3f'''(x_i) + O(h^4) \\
          f(x_i-h) &= f(x_i) - hf'(x_i) + \frac{1}{2}h^2f''(x_i) - \frac{1}{6}h^3f'''(x_i) + O(h^4) \\
          \end{align}
          $$
          <p>We can construct $f'(x_i)$ from these two equations:</p>
          $$
          f'(x_i) = \frac{f(x_i+h) - f(x_i-h)}{2h} - \frac{1}{6}h^2f'''(x_i) + O(h^3)
          $$
          <p class="fragment">The leading error term now scales with $h^2$, therefore this method is second-order accurate.</p>
        </section>

        <section>
          <h2>Higher Order Formulae</h2>
          <p>By using more points, we can construct higher order formulae. For
            example, the following formula is 4th order accurate:</p>
          $$
          f'(x_i) \approx \frac{-f(x_i+2h) + 8f(x_i+h) - 8f(x_i-h) + f(x_i-2h)}{12h}
          $$
        </section>

        <section>
          <h2>Higher Order Formulae</h2>
          <p>How to find formulae like this?</p>
          $$
          \begin{align}
          f(x_i+2h) &= f(x_i) + 2hf'(x_i) + \frac{4}{2}h^2f''(x_i) + \frac{8}{6}h^3f'''(x_i) + O(h^4) \\
          f(x_i+h) &= f(x_i) + hf'(x_i) + \frac{1}{2}h^2f''(x_i) + \frac{1}{6}h^3f'''(x_i) + O(h^4) \\
          f(x_i-h) &= f(x_i) - hf'(x_i) + \frac{1}{2}h^2f''(x_i) - \frac{1}{6}h^3f'''(x_i) + O(h^4) \\
          f(x_i-2h) &= f(x_i) - 2hf'(x_i) + \frac{4}{2}h^2f''(x_i) - \frac{8}{6}h^3f'''(x_i) + O(h^4) \\
          \end{align}
          $$
          <p>Solve for $f'(x_i)$</p>
        </section>

        <section>
          <h2>Finite Difference</h2>
          <p>The same prescription can be used to find higher derivatives too:</p>
          $$
          f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
          $$
          <p>The error on this estimate can be found to be:</p>
          $$
          \varepsilon = -\frac{1}{12}h^2f^{(4)}(x) + O(h^4)
          $$
          <p>So this formula is second-order accurate, similar to central difference.</p>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>
            Back to relaxation method. We replace the derivatives in the ODE using central difference:
          </p>
          $$
          \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} = f\left(x_i,y_i,\frac{y_{i+1}-y_{i-1}}{2h}\right),\quad i = 1,2,\ldots,N-1
          $$
        </section>


        <section>
          <h2>Relaxation Method</h2>
          <p>In general, this is a set of nonlinear equations. It needs to be
            solved iteratively using Newton's method.</p>
          $$
          F_i(\mathbf{y}) = \frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} - f\left(x_i,y_i,\frac{y_{i+1}-y_{i-1}}{2h}\right) = 0
          $$
          <div class="fragment">
            <p>An iteration with Newton's method looks like this:</p>
            $$
            \mathbf{y}_\mathrm{new} = \mathbf{y}_\mathrm{old} - \mathbf{J}^{-1}\mathbf{F}(\mathbf{y}_\mathrm{old})
            $$
            <p>How to write down the Jacobian $\mathbf{J}$?</p>
          </div>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>Since we know the form of function $F_i(\mathbf{y})$, we can take
            the partial derivatives directly:</p>
          $$
          \frac{\partial F_i}{\partial y_j} =
          \begin{cases}
          \displaystyle\frac{1}{h^2} - \frac{\partial f}{\partial y'}\frac{1}{h},\quad j = i + 1 \\
          \displaystyle-\frac{2}{h^2} - \frac{\partial f}{\partial y},\quad j = i \\
          \displaystyle\frac{1}{h^2} + \frac{\partial f}{\partial y'}\frac{1}{h},\quad j = i - 1
          \end{cases}
          $$
          <p class="fragment">This is a <em>tri-diagonal matrix</em>, and its inverse can be
            found relatively easily. </p>
          <p class="fragment">Tri-diagonal matrices are <em>sparse</em>, meaning
            that their elements are mostly zero.</p>
        </section>

        <section>
          <h2>Relaxation Method</h2>
          <p>When there are $M$ equations, this method remains viable, but
            becomes much more complicated. This is because the Jacobian
            $\mathbf{J}$ now has dimension $M(N-1)\times M(N-1)$.</p>
          <div class="fragment">
            <p>$\mathbf{J}$ is still a sparse matrix, with a special
              block-diagonal structure. See Numerical Recipes (3rd Edition) Section
              18.3 for more details.</p>
          </div>
        </section>

        <section>
          <h2>Ordinary Differential Equations</h2>
          <p>We have extensively discussed the numerical methods to solve the
            following type of 1st order ODEs:
            $$
            \frac{d\mathbf{y}}{dx} = f(x, \mathbf{y})
            $$
          </p>
          <p class="fragment">Two ways to pose the problem: IVP or BVP.</p>
        </section>

        <section>
          <h2>Initial Value Problems</h2>
          <p>We have discussed a variety of methods solving IVPs:</p>
          <ul>
            <li class="fragment">Euler and Midpoint Methods</li>
            <li class="fragment">4th-order Runge-Kutta Method, aka RK4</li>
            <li class="fragment">Higher order RK methods, with stepsize control</li>
            <li class="fragment">Implicit methods</li>
            <li class="fragment">Leapfrog integration, symplectic methods</li>
            <li class="fragment">Multistep (Predictor-Corrector) methods</li>
          </ul>
        </section>

        <section>
          <h2>Initial Value Problems</h2>
          <p>IVPs for ODEs is a mature subject. Many methods exist, and many
            readily available packages exist out there.</p>
          <ul>
            <li class="fragment">Python's <code>scipy.solve_ivp</code></li>
            <li class="fragment">Matlab's <code>ode</code> module</li>
            <li class="fragment">Julia's <code>DifferentialEquations.jl</code></li>
            <li class="fragment">Mathematica's <code>NDSolve</code></li>
            <li class="fragment">Many others...</li>
          </ul>
          <p class="fragment">If your goal is simply to quickly get a solution to a given
            equation, then by all means use one of the readily available software packages.</p>
          <p class="fragment">If you need an ODE solver as part of a larger application, then
            it is sometimes very beneficial to know how to implement it by hand.</p>
        </section>

        <section>
          <h2>Boundary Value Problems</h2>
          <p>BVPs can be solved using IVP solvers with the shooting method.</p>
          <p class="fragment">It is also possible to solve BVPs directly by
            solving $N$ coupled nonlinear algebraic equations. Using Newton's
            method, the problem is converted into a linear algebra problem.</p>
          <p class="fragment">Our next logical step is to examine how to deal
            with these linear algebra problems, including solving a system of
            linear equations, decomposing a matrix in various ways,
            diagonalizing a matrix, etc.</p>
        </section>

      </div>
    </div>
    <!-- <script src="js/head.min.js"></script> -->
    <!-- <script>
         head.js(
         "vendor/jquery.min.js",
         // "vendor/three.js/build/three.min.js",
         // "vendor/three.js/examples/js/controls/OrbitControls.js",
         /* "vendor/socket.io-client/dist/socket.io.js", */
         // "vendor/THREE.MeshLine.js",
         /* "vendor/dat.gui.min.js",*/
         "vendor/EventEmitter.js",
         // "js/samples.js",
         );
         </script> -->
    <script src="../dist/reveal.js"></script>
    <script src="../plugin/math/math.js"></script>
    <script src="../plugin/notes/notes.js"></script>
    <script src="../plugin/markdown/markdown.js"></script>
    <script src="../plugin/highlight/highlight.js"></script>

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        transition: "fade",
        slideNumber: true,
        history: true,
        center: true,
        width: "100%",
        height: "100%",
        disableLayout: false,
        navigationMode: "default",
        // minScale: 1,
        // maxScale: 1,
        margin: 0.2,
        padding: 0,
        hash: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          // pass other options into `MathJax.Hub.Config()`
          TeX: { Macros: { RR: "{\\bf R}" } }
        },
        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
      });
      Reveal.configure({
        keyboard: {
          81: null, // Do not respond to Q
          // 32: null, // Do not respond to space
        }
      });
      // Reveal.addEventListener("slidechanged", function(event) {
      //   var foot1 = document.getElementById("footer1");
      //   var foot2 = document.getElementById("footer2");
      //
      //   if (Reveal.getIndices().h === 0) {
      //     foot1.style.display = "none";
      //     foot2.style.display = "none";
      //   } else {
      //     foot1.style.display = "block";
      //     foot2.style.display = "block";
      //   }
      // });
      //
    </script>
  </body>
</html>
